\documentclass[UTF8]{article}
\usepackage{ctex}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{bm}
\usepackage[colorlinks, linkcolor=black]{hyperref}


\title{
    \begin{center}{\Huge \textit{Notes: Neural Networks}}
    %\\{{\itshape This a subtitle}}
    \end{center}}

\begin{document}

    \maketitle
    \tableofcontents

    \newpage


    \section{感知机 Perceptron}
    \textbf{前提：数据是线性可分的！}\\
    点到平面的距离公式：$d=\frac{\left|A x_{0}+B y_{0}+C z_{0}+D\right|}{\sqrt{A^{2}+B^{2}+C^{2}}}$
    \\
    输入：m个样本即$\left(x_{1}^{(0)}, x_{2}^{(0)}, \ldots x_{n}^{(0)}, y_{0}\right),\left(x_{1}^{(1)}, x_{2}^{(1)}, \ldots x_{n}^{(1)}, y_{1}\right), \ldots\left(x_{1}^{(m)}, x_{2}^{(m)}, \ldots x_{n}^{(m)}, y_{m}\right)$，
    标签y是二元类别。
    \\
    目标：找到一个超平面(hyperplane)即$\theta_{0}+\theta_{1} x_{1}+\ldots+\theta_{n} x_{n}=0$，让其中一种类别的样本都满足$\theta_{0}+\theta_{1} x_{1}+\ldots+\theta_{n} x_{n}>0$；
    而另一种类别的样本都满足$\theta_{0}+\theta_{1} x_{1}+\ldots+\theta_{n} x_{n}<0$，从而线性可分！\textbf{简化写法：}增加一个特征$x_0=1$，所以有超平面$\sum_{i=0}^{n} \theta_{i} x_{i}=0$
    \textbf{向量表示：}$$\theta_{(n+1) \times 1} \cdot x_{1 \times (n+1)}= 0$$
    \\
    感知机模型定义: $$y=\operatorname{sign}(\theta \bullet x)$$
    $$\operatorname{sign}(x)=\left\{\begin{array}{ll}{-1} & {x<0} \\ {1} & {x \geq 0}\end{array}\right.$$
    \\
    Gram矩阵(可以看成没有减去均值的协方差矩阵), 感知机\textbf{原始形式以及对偶形式}

    \section{DNN反向传播}
    \subsection{要解决的问题}
    在监督学习中往往要利用一个loss function来度量当前模型从输入得到的结果与真实值的误差。通过最小化这个误差，把整个模型往最好的方向调整即得到最好的weight和b;
    而最小化的方法便有梯度下降，牛顿法，拟牛顿法等...

    \section{RNN BPTT(back propagation through time)}
    模型定义参考\href{https://www.cnblogs.com/pinard/p/6509630.html}{刘建平RNN推导}

    \subsection{前向传播}
    \begin{enumerate}
        \item 对于任意一个序列索引号t：$h^{(t)}=\sigma\left(z^{(t)}\right)=\sigma\left(U x^{(t)}+W h^{(t-1)}+b\right)$，这里的$\sigma$是tanh函数
        \item 序列索引号t时模型的输出$o^{t}$的表达式为：$o^{(t)}=V h^{(t)}+c$，$c，b$一样，都是偏置向量
        \item 最终在序列索引号t时预测输出$\hat{y}^{(t)}=\sigma\left(o^{(t)}\right)$，这里的$\sigma$是softmax函数
    \end{enumerate}

    \subsection{反向传播}
    通过梯度下降法的一轮迭代，得到模型参数$U,W,V,b,c$，\textbf{这些参数在序列的各个位置是共享的}，BP时更新的是相同的参数！

    由于在序列的每个位置都有损失函数$L^{(t)}=-{y^{(t)}}^T log{\hat{y}^{(t)}}$，因此最终的损失$L$为：$L=\sum_{t=1}^{\tau} L^{(t)}$

    下面的推导详见草稿纸笔记。。。


\end{document}