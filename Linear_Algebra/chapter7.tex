\section{Singular Values Decomposition}
提前介绍后面要用到的性质。

[矩阵$\bm{A^TA}$和$\bm{AA^T}$(任意维度)的性质及证明]\url{https://blog.csdn.net/Europe233/article/details/86720864}

性质：$\bm{A^TA}$和$\bm{AA^T}$具有相同的非零特征值

证明1：\url{https://blog.csdn.net/Europe233/article/details/86727078}

证明2：
\begin{enumerate}
    \item 假设$\bm{x} \neq \bm{0}$，$\bm{A}^T\bm{Ax}=\lambda \bm{x}$，等式左右两边同时左乘一个$\bm{A}$
    得$\bm{\rightarrow} (\bm{AA}^T)\bm{Ax}=\lambda \bm{Ax}$；
    \item 同理，假设$\bm{x} \neq \bm{0}$，$\bm{AA}^T\bm{x}=\lambda \bm{x}$，左右两边同时左乘一个$\bm{A^T}$，
    得$\bm{\rightarrow} (\bm{A}^T\bm{A})\bm{A}^T\bm{x}=\lambda \bm{A^T}\bm{x}$。
    \item 经检验，维度是匹配的。这说明在$\lambda \neq 0$时上述式子成立，也即$\bm{A^TA}$和$\bm{AA^T}$具有相同的非零特征值。
\end{enumerate}

\subsection{SVD的目的}
从空间变换的角度出发
\begin{itemize}
    \item 对称阵的谱分解(特征值分解)：通过矩阵$\bm{A}$将一组基映射到同一组基。
    \item SVD的目的是：通过任意矩阵$\bm{A}_{m\times n}$将一组单位正交基映射到另一组单位正交基。
\end{itemize}

SVD的两种表示(下面的向量和矩阵都没有加粗，自行辨识)：
$$A v_{i}=\sigma_{i} u_{i}$$
$$\begin{array}{c}
(m \text { by } n)(n \text { by } r) \\
A V_{r}=U_{r} \Sigma_{r} \\
(m \text { by } r)(r \text { by } r)
\end{array}
\\
A\Bigg[\begin{array}{lll}
v_{1} & \cdots & v_{r}
\end{array}\Bigg]=\Bigg[\begin{array}{lll}
u_{1} & \cdots & u_{r}
\end{array}\Bigg]\left[\begin{array}{cccc}
\sigma_{1} & & & \\
& \cdot & \\
& & \cdot & \\
& & & \sigma_{r}
\end{array}\right]$$


书上7.2的直接给出向量$u,v$是从四个基本子空间的正交单位基而来。

这里给出个人理解：
\begin{itemize}
    \item 直接从目标的表达式$\bm{A V=U \Sigma}$(此处$\bm{V,U,\Sigma}$为方阵)看出，
    每个向量$\bm{u}_i$是矩阵$\bm{A}$的列向量的线性组合，所以$\bm{u}_i$处于$\bm{A}$的\textbf{列空间}中！
    \item 倘若按照SVD的目的，从一组单位正交基映射到另一组单位正交基，那么矩阵$\bm{U,V}$便都是正交矩阵，
    所以对目标表达式左乘一个$U^T$，右乘一个$V^T$然后再转置得到：$\bm{A}^T\bm{U} = \bm{V\Sigma}$。
    由此便可以按列向量展开，然后看出每个$\bm{v}_i$向量都是矩阵$A$的行向量的线性组合，即$\bm{v}_i$处于
    $\bm{A}$的\textbf{行空间}中。
    \item 所以书上一开始就直接去$\bm{A}$的列空间和行空间里面去找规范正交基
    \item 后面分别找零空间和左零空间的规范正交基是因为要找满足分别与先前两组规范正交基正交的另外两组基，
    并且恰好分别能将其扩展为方阵！
\end{itemize}

\url{https://www.zhihu.com/question/22237507}、\url{https://blog.csdn.net/zhongkejingwang/article/details/43053513}，
的推导过程：
\\
\textbf{假设}找到了一组正交的基$\left\{v_{1}, v_{2}, \ldots, v_{n}\right\}$，
通过矩阵A将这组正交基映射为$\left\{A v_{1}, A v_{2}, \ldots, A v_{n}\right\}$，令映射后的向量两两正交即：
$\left(\mathrm{A} v_{i}\right)^{T} A v_{j}=v_{i}^{T} A^{T} \mathrm{A} v_{j}=0$。
根据假设，存在$v_{i}^{T} v_{j}=0$，由此看出，如果\textbf{选用矩阵$A^TA$这个实对称阵的特征向量作为$v_i$的话，就满足要求}，
也即$v_{i}^{T} A^{T} \mathrm{A} v_{j}= v_{i}^{T} \lambda_{j} v_{j} =\lambda_{j} v_{i}^{T} v_{j} =\lambda_{j} v_{i}, v_{j}=0$。
所以这里便找到了另一组对应的正交基$\left\{A v_{1}, A v_{2}, \ldots, A v_{n}\right\}$！

然后将映射后的正交基单位化：
\begin{itemize}
    \item 因为$(A v_{i})^T A v_{i}=v_{i}^T (A^TA)v_i = v_{i}^T \lambda_{i} v_{i}=\lambda_{i}$($v_i$是正交单位向量)
    \item 所以有$\left||A v_{i}\right||^{2}=\lambda_{i} \geq 0$
    \item 取单位向量：$u_{i}=\dfrac{A v_{i}}{\left||A v_{i}\right||}=\dfrac{1}{\sqrt{\lambda_{i}}} A v_{i}$
\end{itemize}
由此可得，$A v_{i}=\sigma_{i} u_{i}, \sigma_{i}($奇异值$)=\sqrt{\lambda_{i}}, 0 \leq i \leq \mathrm{k}, \mathrm{k}=\operatorname{Rank}(\mathrm{A})$

\textbf{注意}：对$u_{i}=\dfrac{A v_{i}}{\left||A v_{i}\right||}=\dfrac{1}{\sqrt{\lambda_{i}}} A v_{i}$左右两边同时左乘上$AA^T$
    得
\begin{itemize}
    \item $AA^Tu_{i}=\dfrac{1}{\sqrt{\lambda_{i}}} AA^TA v_{i}$
    \item 即$AA^Tu_{i}=\dfrac{1}{\sqrt{\lambda_{i}}} A(A^TA) v_{i}=\dfrac{1}{\sqrt{\lambda_{i}}} A \lambda_i v_{i}$
    \item 即$AA^Tu_{i}=\lambda_i \dfrac{1}{\sqrt{\lambda_{i}}} A  v_{i}$
    \item 即$AA^Tu_{i}=\lambda_i u_i$(根据假设定义$A v_{i}=\sqrt{\lambda_{i}} u_{i}$)
\end{itemize}
\textbf{所以，向量$u_i$是矩阵$AA^T$的特征向量，且特征值相同(对应最开始$AA^T, A^TA$的性质)}

将SVD用向量写出来：$A=U \Sigma V^{\mathrm{T}}=u_{1} \sigma_{1} v_{1}^{\mathrm{T}}+\cdots+u_{r} \sigma_{r} v_{r}^{\mathrm{T}}$，
这便是$\bm{A}$的满秩分解，将$\bm{A}$分解为只有$rank(\bm{A})$个矩阵之和！当然$\bm{U,V}$ 扩展成方阵也是，
新增进去的向量不起作用，以及$\bm{\Sigma}$多出来的部分里面值为0。

\subsection{瑞利熵(Rayleigh quotient)}
实数上的定义：
$$r(\boldsymbol{x})=\dfrac{\boldsymbol{x}^{\mathrm{T}} S \boldsymbol{x}}{\boldsymbol{x}^{\mathrm{T}} \boldsymbol{x}}$$
可以参考\url{https://www.cnblogs.com/xingshansi/p/6702188.html?utm_source=itdadao&utm_medium=referral}A-普通瑞利熵
部分，得出对该式求得最大值时的条件就是$S x=r(x) x$，也就是说当$r(\bm{x})$为$S$的特征值，$\bm{x}$为$S$的特征向量时满足要求！
与书上的表述一致。

\subsection{PCA by SVD}
\textbf{PCA详细推导见}\url{https://www.cnblogs.com/pinard/p/6239403.html}。
本小节主要是介绍SVD在PCA中的应用。

样本中心化：给定一个数据矩阵$\bm{A_0},size=(m \times n); n \rightarrow samples, m \rightarrow variables$。
对$\bm{A}_0$的每一行减去该行的均值，便得到矩阵$\bm{A}$。可以求得，现在矩阵$\bm{A}$的每行的均值
为0。
然后根据\textbf{样本协方差矩阵}定义(sample covariance matrix，也叫自协方差矩阵)的定义，得到$\bm{S}=\dfrac{\bm{A} \bm{A}^{\mathrm{T}}}{n-1}$。
以上定义以及推导参考\url{https://blog.csdn.net/Happy_code666/article/details/101716869}。

根据方差以及协方差的定义，可知对于已经样本中心化的矩阵$\bm{A}_{2\times 2}$有：
\begin{itemize}
    \item $\left(A A^{\mathrm{T}}\right)_{11}$ and $\left(A A^{\mathrm{T}}\right)_{22}$表示
    该两个样本的方差$s_{1}^{2}, s_{2}^{2}$
    \item $\left(A A^{\mathrm{T}}\right)_{12}$表示该两个样本间的协方差$s_{12}$(等同于$s_{21}$)
\end{itemize}
Example1 表明通过对矩阵A利用其样本协方差矩阵S做SVD分解，可以找到A的每列样本在相应的空间中是如何分布的。
\begin{itemize}
    \item 样本矩阵$A_0$的总方差T等于$tr(S)$，同时也等于S的特征值之和。
    Total variance $T=\sigma_{1}^{2}+\cdots+\sigma_{m}^{2}=s_{1}^{2}+\cdots+s_{m}^{2}=$ trace $($diagonal sum$)$
    \item S的第一个特征向量$\bm{u}_1$指明了数据分布的主要方向，该方向占了总方差的$\sigma_{1}^{2} / T$部分。
    \item 当其他部分很小时，对于数据在对应空间的分布就不起明显的作用了。因为对$A$做SVD分解得到的起主要作用的奇异向量
    有$R$个，所以主要有$R$个方向。
\end{itemize}

注意：在Chpater4中提到最小二乘法是利用\textbf{竖直距离}(vertical offsets)来拟合直线，而PCA这里是利用\textbf{垂直距离}(perpendicular offsets)来拟合直线，
也叫垂直最小二乘(Perpendicular Least Squares)。

\subsection{SVD的几何理解}

(orthogonal) x (diagonal) x (orthogonal) = (rotation) x (stretching) x (rotation)

好几个性质：A的范数norm等。。。详见书本7.4

\subsubsection{伪逆$\mathbf{A}^+$}

对于前面提到的SVD，有任意矩阵$\bm{A}_{m\times n}$乘上它的行空间正交基向量$\bm{v}_i$得到列空间中的$\sigma_i \bm{u}_i$。

若是当它的逆矩阵$\bm{A}^{-1}$存在，则上述定义式子和性质则要反过来，即从$A \boldsymbol{v}=\sigma \boldsymbol{u}$ 变成
$A^{-1} \boldsymbol{u}=\boldsymbol{v} / \sigma$，且$A^{-1}$的奇异值为$1 / \sigma$，
向量$\bm{u}'s$在$\bm{A}^{-1}$的行空间中，$\bm{v}'s$在$\bm{A}^{-1}$的列空间中。

然而\textbf{不需要“逆矩阵$\bm{A}^{-1}$存在”}这个前提，也存在一个矩阵$\bm{A}^{+}_{n\times m}$能够满足$A^{+} \boldsymbol{u}_i=\boldsymbol{v}_i / \sigma$。

$\bm{A}_{m\times n}$的伪逆矩阵：
$$A^{+}=V \Sigma^{+} U^{\mathrm{T}} =
\Biggl[\boldsymbol{v}_{1} \cdots \boldsymbol{v}_{r} \cdots \boldsymbol{v}_{n}\Biggr]_{n\times n}
\left[\begin{array}{cccc}
\sigma_{1}^{-1} & & & \\
& \ddots & \\
& & \sigma_{r}^{-1} & \\
& & & 
\end{array}\right]_{n \times m}
\Biggl[\boldsymbol{u}_{1} \cdots \boldsymbol{u}_{r} \cdots \boldsymbol{u}_{m}\Biggr]^{\mathrm{T}}_{m\times m}
$$

如果$\bm{A}^{-1}$存在的话，则$\bm{A}^{-1}=\bm{A}^{+}$，即$m=n=r$，相当于将原来的$U \Sigma V^{\mathrm{T}}$ 转换成 $V \Sigma^{-1} U^{\mathrm{T}}$。
只有当矩阵$\bm{A}$的秩$r<m$或$r<n$时，即它没有逆矩阵时才需要伪逆$\bm{A}^{+}$，且$\bm{A}^{+}$
有相同的秩$r$。
$$A^{+} \boldsymbol{u}_{i}=\frac{1}{\sigma_{i}} \boldsymbol{v}_{i} \quad  for \quad i \leq r \quad and \quad A^{+} \boldsymbol{u}_{i}=\mathbf{0} \quad for \quad i>r$$
将先前的矩阵乘的结果写出来就是上面的式子。

$A A^{+}$ 和 $A^{+} A$的行列空间互换了。

$$\Sigma^{+} \Sigma=
\left[\begin{array}{ll} \bm{I} & \bm{0} \\ \bm{0} & \bm{0}\end{array}\right]
$$
$A A^{+}$ and $A^{+} A$是投影矩阵，看Fig 7.6
\begin{itemize}
    \item $A A^{+}$把向量投影到$A$的列空间
    \item $A^{+} A$把向量投影到$A$的行空间
\end{itemize}

\subsubsection{列向量相关的最小二乘}
在chapter4.3中，矩阵$A$的列向量是线性无关的，但是如果改成线性相关，那么利用等式
$A^{\mathrm{T}} A \boldsymbol{x}=\boldsymbol{A}^{\mathrm{T}} \boldsymbol{b}$
求最小二乘解的时候，$\bm{x}$就会有无穷多个解，无法给出最好的解。

可以利用伪逆来给出最短的解：$\bm{x}^{+}=A^{+} \bm{b}$(这里最短指的是向量的模最小)。